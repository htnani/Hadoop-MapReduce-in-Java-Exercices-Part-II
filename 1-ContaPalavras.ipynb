{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código desenvolvido em Java para criação da aplicação para contar palavras. Projeto inicial chamado ContaPalavras contendo a estrutura-base das classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Desenvolvimento das 3 classes Mapper, Reducer e Driver para implementação da aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Driver ContaPalavrasDriver.java\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "\n",
    "public class ContaPalavrasDriver {\n",
    "\n",
    "\t\n",
    "\t\n",
    "\tpublic static void main(String[] args) throws Exception {\n",
    "\t\t\n",
    "\t\tConfiguration conf = new Configuration();\n",
    "\t\t// o nome \"contapalavrasdriver\" serve para quando verificar o log identificar a qual job se refere\n",
    "\t\tJob job = Job.getInstance(conf, \"contapalavrasdriver\");\n",
    "\t\t\n",
    "\t\tjob.setJarByClass(ContaPalavrasDriver.class);\n",
    "\t\tjob.setMapperClass(ContaPalavrasMap.class);\n",
    "\t\tjob.setReducerClass(ContaPalavrasReduce.class);\n",
    "\t\tjob.setOutputKeyClass(Text.class);// titpo de dado da chave\n",
    "\t\tjob.setOutputValueClass(IntWritable.class);\n",
    "\t\t//job.setMapOutputKeyClass(theClass); indica tipo de dado do map\n",
    "\t\t//job.setInputFormatClass(cls); estamos usando a default, que é texto\n",
    "\t\tFileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "\t\t\n",
    "\t\t\t    \n",
    "\t  } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Mapper ContaPalavrasMap.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "                                              // tipo da chave e valor de entrada, tipo da chave e valor da saida\n",
    "public class ContaPalavrasMap extends Mapper<Object, Text, Text, IntWritable>{\n",
    "\t// final indica que não vou alterar a variavel\n",
    "\tprivate final static IntWritable numeroum = new IntWritable(1);\n",
    "\t// context serve para escrever os metodos intermediarios\n",
    "    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n",
    "    \tStringTokenizer itr = new StringTokenizer(value.toString().replaceAll(\"[^a-zA-Z ]\", \"\").toLowerCase());\n",
    "    \twhile (itr.hasMoreTokens()) {\n",
    "    \tcontext.write(new Text(itr.nextToken()), numeroum);\n",
    "    \t}\n",
    "    }   \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desesenvolvimento da classe Reducer ContaPalavrasReduce.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "\n",
    "public class ContaPalavrasReduce extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "\n",
    "\tpublic void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "\t\tint soma = 0;\n",
    "\t\tfor (IntWritable val : values) {\n",
    "\t\tsoma += val.get();\n",
    "\t\t}\n",
    "\t\tcontext.write(key, new IntWritable(soma));\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Geração do arquivo JAR da aplicação e definição do path:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: /home/cloudera/Documents/contapalavras.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Execução do job JAR no ambiente Hadoop e consumo do arquivo bases/base_tw.txt já armazenado no HDFS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " hadoop jar ~/Documents/contapalavras.jar ContaPalavrasDriver bases/base_tw.txt twitter/saida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Visualização do resultado da aplicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs dfs -text twitter/saida/part-r-00000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SciJava",
   "language": "groovy",
   "name": "scijava"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": "",
   "mimetype": "",
   "name": "scijava",
   "nbconverter_exporter": "",
   "pygments_lexer": "groovy",
   "version": "1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
