{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código desenvolvido em Java para criação da aplicação que utilize a base de dados compras.txt e lista para cada cidade o tipo de produto mais vendido. Para realizar esta tarefa foi desenvolivido dois pares de MapReduce, o primeiro armazena cidade e produto e uma mesma string (chave) dividida por & e coloca no array (valor) um 1 para cada vez que se repete e conta a quantidades de 1 no array de cada chave cidade&produto. A segunda iteração MapReduce lista o produto mais vendido para cada cidade. Projeto inicial chamado contamaxcidadeproduto contendo a estrutura-base das classes da primeira iteração e contamaxcidadeprodutosecond para a segunda iteração MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Desenvolvimento das 3 classes Mapper, Reducer e Driver para implementação da 1º iteração MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Driver ContaMaxCidadeProdutoDriver.java\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class ContaMaxCidadeProdutoDriver {\n",
    "\n",
    "\tpublic static void main(String[] args) throws Exception {\n",
    "\n",
    "\t\tConfiguration conf = new Configuration();\n",
    "\t\tJob job = Job.getInstance(conf, \"contamaxcidadeprodutodriver\");\n",
    "\t\tjob.setJarByClass(ContaMaxCidadeProdutoDriver.class);\n",
    "\t\tjob.setMapperClass(ContaMaxCidadeProdutoMapper.class);\n",
    "\t\tjob.setMapOutputKeyClass(Text.class);\n",
    "\t\tjob.setMapOutputValueClass(IntWritable.class);\n",
    "\t\tjob.setReducerClass(ContaMaxCidadeProdutoReducer.class);\n",
    "\t\tjob.setOutputKeyClass(Text.class);\n",
    "\t\tjob.setOutputValueClass(IntWritable.class);\n",
    "\t\tFileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Mapper ContaMaxCidadeProdutoMapper.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "public class ContaMaxCidadeProdutoMapper extends Mapper<Object, Text, Text, IntWritable> {\n",
    "\n",
    "\tprivate final static Text cidadeproduto = new Text();\n",
    "\n",
    "\tpublic void map(Object key, Text value, Context context) throws IOException, InterruptedException {\t\t\n",
    "\t\tString[] linha=value.toString().split(\";\");\t\t\n",
    "\t\t//armazena cidade e produto e uma mesma string (chave) dividida por & e coloca no array (valor) um 1 para cada vez que se repete\n",
    "\t\tcidadeproduto.set(linha[2] +\"&\"+linha[3]);\n",
    "\t\tcontext.write(cidadeproduto, new IntWritable(1));\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desesenvolvimento da classe Reducer ContaMaxCidadeProdutoReducer.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "public class ContaMaxCidadeProdutoReducer extends\tReducer<Text, IntWritable, Text, IntWritable> {\n",
    "\t\n",
    "\tpublic void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n",
    "\t\tint countValue = 0;\n",
    "\t\t\n",
    "\t\t//conta a quantidades de 1 no array de cada chave cidade&produto\n",
    "\t\tfor (IntWritable value : values) {\n",
    "\t\t\tcountValue += value.get();\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tcontext.write(key, new IntWritable(countValue));\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Geração do arquivo JAR da aplicação e definição do path:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: /home/cloudera/Documents/contamaxcidadeproduto.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Execução do job JAR no ambiente Hadoop e consumo do arquivo bases/compras.txt já armazenado no HDFS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar ~/Documents/contamaxcidadeproduto.jar ContaMaxCidadeProdutoDriver bases/compras.txt saida/cidade_produto_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Desenvolvimento das 3 classes Mapper, Reducer e Driver para implementação da 2º iteração MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Driver ContaMaxCidadeProdutoSecondDriver.java\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class ContaMaxCidadeProdutoSecondDriver {\n",
    "\n",
    "\tpublic static void main(String[] args) throws Exception {\n",
    "\n",
    "\t\tConfiguration conf = new Configuration();\n",
    "\t\tJob job = Job.getInstance(conf, \"contamaxcidadeprodutoseconddriver\");\n",
    "\t\tjob.setJarByClass(ContaMaxCidadeProdutoSecondDriver.class);\n",
    "\t\tjob.setMapperClass(ContaMaxCidadeProdutoSecondMapper.class);\n",
    "\t\tjob.setMapOutputKeyClass(Text.class);\n",
    "\t\tjob.setMapOutputValueClass(Text.class);\n",
    "\t\tjob.setReducerClass(ContaMaxCidadeProdutoSecondReducer.class);\n",
    "\t\tjob.setOutputKeyClass(Text.class);\n",
    "\t\tjob.setOutputValueClass(IntWritable.class);\n",
    "\t\tFileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Mapper ContaMaxCidadeProdutoSecondMapper.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "public class ContaMaxCidadeProdutoSecondMapper extends Mapper<Object, Text, Text, Text> {\n",
    "\n",
    "\tpublic void map(Object key, Text value, Context context) throws IOException, InterruptedException{\n",
    "\t    //recebe a linha com duas strings separadas por \\t, uma string cidade&produto e outra de soma de 1 para cada chave\n",
    "\t\t//divide a linha pelo \\t \n",
    "\t\tString[] linha = value.toString().split(\"\\t\");\n",
    "\t    //divide a primeira string da linha pelo & e armazena cidade\n",
    "\t\tString cidade = linha[0].split(\"&\")[0];\n",
    "\t\t//divide a primeira string da linha pelo & e armazena produto\n",
    "\t    String produto = linha[0].split(\"&\")[1];\n",
    "\t    //armazena a segunda strin da linha, que é soma dos valores 1\n",
    "\t    String count = linha[1];\n",
    "\t    \n",
    "\t    //envia para o reducer cada chave cidade com o array [produto&valor1, produto&valor2, produto&valor3, ...] \n",
    "\t    context.write(new Text(cidade), new Text(produto + \"&\" + count));     \n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desesenvolvimento da classe Reducer ContaMaxCidadeProdutoSecondReducer.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "public class ContaMaxCidadeProdutoSecondReducer extends\tReducer<Text, Text, Text, Text> {\n",
    "\t\n",
    "\tpublic void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException{\n",
    "\t    int maxVal = Integer.MIN_VALUE;\n",
    "\t    String maxProd = \"None\";\n",
    "\t    \n",
    "\t    //iterar sobre todos os pruduto&valor de cada cidade/chave\n",
    "\t    for (Text value : values) {\n",
    "\t    \t//dividir a string pruduto&valor pelo & e armazena no array ss [produto,valor]\n",
    "\t        String ss []= value.toString().split(\"&\");        \n",
    "\t        //armazena em cnt o valor do produto\n",
    "\t        int cnt = Integer.parseInt(ss[1]);\n",
    "\t        //testa se cnt é maior que o max armazenado, se sim, armazena o novo valor max e o produto\n",
    "\t        if(cnt > maxVal){\n",
    "\t            maxVal = cnt;\n",
    "\t            maxProd = ss[0];\n",
    "\t        }\n",
    "\t    }\n",
    "\t    context.write(key, new Text(maxProd+\" \" +maxVal));\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-Geração do arquivo JAR da aplicação e definição do path:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: /home/cloudera/Documents/contamaxcidadeprodutosecond.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-Execução do job JAR no ambiente Hadoop e consumo do arquivo saida/cidade_produto_count/part-r-00000 resultado do Job anterior e já armazenado no HDFS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar ~/Documents/contamaxcidadeprodutosecond.jar ContaMaxCidadeProdutoSecondDriver saida/cidade_produto_count/part-r-00000  saida/cidade_produto_max"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SciJava",
   "language": "groovy",
   "name": "scijava"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": "",
   "mimetype": "",
   "name": "scijava",
   "nbconverter_exporter": "",
   "pygments_lexer": "groovy",
   "version": "1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
