{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código desenvolvido em Java para criação da aplicação que conta o número de hashtags e retorna a n quantidade de hashtags com maior frequência, onde n é um parâmetro especificado pelo usuário da aplicação, em uma base de dados do twitter. A aplicação também verifica se a pasta destino do Job já existe, se sim, a pasta é excluida. Projeto inicial chamado TopNHashTagParam contendo a estrutura-base das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Desenvolvimento das 3 classes Mapper, Reducer e Driver para implementação da aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//// Desenvolvimento da classe Driver TopNHashTagDriver.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.FileSystem;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.NullWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "\n",
    "public class TopNHashTagDriver {\n",
    "\tpublic static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException{\n",
    "\t\tConfiguration conf = new Configuration();\n",
    "\t\t//recebendo o parametro qtd do usuario\n",
    "\t\tconf.set(\"qtd\", args[2]);\n",
    "\t\t\n",
    "\t\t//verica se a pasta destino já existe, se sim, a pasta será excluida\n",
    "\t\tconf.set(\"fs.defaultFS\",\"hdfs://localhost:8020/\");\n",
    "\t\tFileSystem dfs = FileSystem.get(conf);\n",
    "\t\tString nomeDir = args[1];\n",
    "\t\tPath caminhoDir = new Path(dfs.getWorkingDirectory()+\"/\"+nomeDir);\n",
    "\t\t\n",
    "\t\tif(dfs.exists(caminhoDir)){\n",
    "\t\t\tSystem.out.println(\"Pasta destino já existe e será excluida\");\n",
    "\t\t\tdfs.delete(caminhoDir);\n",
    "\t\t}\n",
    "\n",
    "\t\tJob job = Job.getInstance(conf);\n",
    "\t\tjob.setJarByClass(TopNHashTagDriver.class);\n",
    "\t\tjob.setMapperClass(TopNHashTagMapper.class);\n",
    "\t\tjob.setReducerClass(TopNHashTagReducer.class);\n",
    "\t\tjob.setOutputKeyClass(NullWritable.class);\n",
    "\t\tjob.setOutputValueClass(Text.class);\n",
    "\t\tjob.setNumReduceTasks(1);\n",
    "\n",
    "\t\tFileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "\t\tFileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "\t\t\n",
    "\n",
    "\t\t\n",
    "\t\tSystem.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desenvolvimento da classe Mapper TopNHashTagMapper.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.TreeMap;\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.io.NullWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "\n",
    "public class TopNHashTagMapper extends Mapper<Object, Text, NullWritable, Text>{\n",
    "\tprivate TreeMap<Integer, Text> topN = new TreeMap<>();\n",
    "\n",
    "\n",
    "\tpublic void map(Object key, Text value, Context context)throws IOException, InterruptedException {\n",
    "\t\t//capturando o parametro qtd informado pelo usuario\n",
    "\t\tConfiguration conf = context.getConfiguration();\n",
    "\t\tint qtd = Integer.parseInt(conf.get(\"qtd\"));\n",
    "\n",
    "\t\tString[] listaHashtags = value.toString().toLowerCase().split(\"\\t\") ;\n",
    "\n",
    "\t\t//Entada: hashtag - qtd de hashtags\n",
    "\t\tif (listaHashtags.length < 2) {\n",
    "\t\t\treturn;\n",
    "\t\t}\n",
    "\n",
    "\t\ttopN.put(Integer.parseInt(listaHashtags[1].toLowerCase()), new\n",
    "\t\t\t\tText(value));\n",
    "\t\tif (topN.size() > qtd) {\n",
    "\t\t\ttopN.remove(topN.firstKey());\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tprotected void cleanup(Context context) throws IOException, InterruptedException {\n",
    "\t\tfor (Text t : topN.values()) {\n",
    "\t\t\tcontext.write(NullWritable.get(), t);\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Desesenvolvimento da classe Reducer TopNHashTagReducer.java\n",
    "\n",
    "import java.io.IOException;\n",
    "import java.util.TreeMap;\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.io.NullWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "\n",
    "public class TopNHashTagReducer extends Reducer<NullWritable, Text, NullWritable,Text> {\n",
    "\tprivate TreeMap<Integer, Text> topN = new TreeMap<>();\n",
    "\t\n",
    "\tpublic void reduce(NullWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n",
    "\t\tfor (Text value : values) {\n",
    "\t\t\t String[] hashtags = value.toString().toLowerCase().split(\"\\t\") ;\n",
    "\n",
    "\t\t\t //capturando o parametro qtd informado pelo usuario\n",
    "\t\t\t Configuration conf = context.getConfiguration();\n",
    "\t\t\t int qtd = Integer.parseInt(conf.get(\"qtd\"));\n",
    "\t\t\t topN.put(Integer.parseInt(hashtags[1].toLowerCase()),\n",
    "\t\t\t new Text(value));\n",
    "\t\t\t if (topN.size() > qtd) {\n",
    "\t\t\t topN.remove(topN.firstKey());\n",
    "\t\t}\n",
    "\t}\n",
    "\t\tfor (Text word : topN.descendingMap().values()) {\n",
    "\t\t\tcontext.write(NullWritable.get(), word);\n",
    "\t\t}\n",
    "\t}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Geração do arquivo JAR da aplicação e definição do path:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo: /home/cloudera/Documents/topnhashtagparam.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Execução do job JAR no ambiente Hadoop, consumo do arquivo bases/base_tw.txt já armazenado no HDFS e passado o parametro 5 para retorno das 5 hashtags mais citadas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar ~/Documents/TopNHashTagParam.jar TopNHashTagDriver bases/base_tw.txt twitter/saida 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-Visualização do resultado da aplicação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs dfs -text twitter/saida/part-r-00000"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "SciJava",
   "language": "groovy",
   "name": "scijava"
  },
  "language_info": {
   "codemirror_mode": "groovy",
   "file_extension": "",
   "mimetype": "",
   "name": "scijava",
   "nbconverter_exporter": "",
   "pygments_lexer": "groovy",
   "version": "1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
